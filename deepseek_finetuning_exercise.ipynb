{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3858fb6a-8aca-4fec-a28f-4e975838bbf7",
   "metadata": {},
   "source": [
    "1. Install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8aea6-918e-418c-adb9-84da0864e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223798c-2c05-4dda-8814-3065e7719753",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053032ab-baee-4689-b461-d766ef4251ba",
   "metadata": {},
   "source": [
    "2. Huggin Face login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f7dc1-443b-4e56-8d34-645c71635bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face login\n",
    "from huggingface_hub import login\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter Token value\n",
    "hf_token = getpass('Enter Hugging Face Token: ')\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186dff9-5bf2-48af-9d46-1f8ed6c4475e",
   "metadata": {},
   "source": [
    "3. Loading Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a98d3-8937-4846-ad79-47d21c82836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Define Model Settings\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "print(\"Setup is complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb64ab3-5b04-4bd3-a2b6-406031a0495c",
   "metadata": {},
   "source": [
    "4. Model inference step before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26882586-c1b1-4426-9bdb-efa6b69e5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are pairs of instructions that describe the task and input that provides additional context.\n",
    "# Write a response that appropriately completes the request.\n",
    "# Before answering, think carefully about the question and create a step-by-step thought process to ensure a logical and accurate response.\n",
    "\n",
    "# ### Instructions:\n",
    "# You are a healthcare professional with advanced knowledge of clinical reasoning, diagnosis, and treatment planning.\n",
    "# Please answer the following healthcare questions.\n",
    "\n",
    "\n",
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
    "Please answer the following medical question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45c7be-7ed2-4daa-9f9f-febbc5bdfbd6",
   "metadata": {},
   "source": [
    "5. Inference answer generation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7d0c1-5e80-43fa-a82a-2c800c6f6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"A 55-year-old extremely obese man experiences weakness, sweating, tachycardia, confusion, and headache when fasting for a few hours, which are relieved by eating. What disorder is most likely causing these symptoms?\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ace2e2-f750-45fb-853c-d4942df4d070",
   "metadata": {},
   "source": [
    "6. Loading and processing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb09551-9519-4823-92ec-66a119d94a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are pairs of instructions that describe the task and input that provides additional context.\n",
    "# Write a response that appropriately completes the request.\n",
    "# Before answering, think carefully about the question and create a step-by-step thought process to ensure a logical and accurate response.\n",
    "\n",
    "# ### Instructions:\n",
    "# You are a healthcare professional with advanced knowledge of clinical reasoning, diagnosis, and treatment planning.\n",
    "# Please answer the following healthcare questions.\n",
    "\n",
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
    "Please answer the following medical question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55720393-2e1b-417e-8d53-3955fc229b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e886d-f1b1-4d65-935b-78e42871028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:700]\",trust_remote_code=True)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f151e5-7c92-4c74-987f-7a199b1e4736",
   "metadata": {},
   "source": [
    "7. Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570616e-e2f2-4e69-9ca5-b2d2a9e32b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b675c0-dfce-4832-a990-83ec1cb8bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc0e99-97dd-485a-801c-731f7bb9dda5",
   "metadata": {},
   "source": [
    "8. Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e64088-59cd-4266-8afb-065eb659f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "# Disable all wandb related features\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "# Run training\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3e284-ea8a-4013-bae2-50bd1f744339",
   "metadata": {},
   "source": [
    "9. Model inference after fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b13cb2-fa1b-4789-affa-02fb1542c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"A 55-year-old extremely obese man experiences weakness, sweating, tachycardia, confusion, and headache when fasting for a few hours, which are relieved by eating. What disorder is most likely causing these symptoms?\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4bbe29-461c-48c7-8a9c-835298bf1c2c",
   "metadata": {},
   "source": [
    "10. Save the Model on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ae5cc-710f-4a03-a238-ad5c7b2a0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_local = \"DeepSeek-R1-medicalassistant\"\n",
    "model.save_pretrained(new_model_local)\n",
    "tokenizer.save_pretrained(new_model_local)\n",
    "\n",
    "model.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804f817-a3da-4b56-bff1-cd0dedc88ab4",
   "metadata": {},
   "source": [
    "11. Save the Model on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4997659-8207-43a7-9ad7-cce780a493d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login, create_repo\n",
    "import os\n",
    "\n",
    "# 1. Setting environment variables\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_your_token\"\n",
    "login()  # Log in with token\n",
    "\n",
    "# 2. Create a new repository and upload it\n",
    "repo_name = \"<your hugging face repo for uploading>\"\n",
    "#create_repo(repo_name)\n",
    "model.push_to_hub_gguf(repo_name, tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4512019-9433-4cf6-afd7-b21ba61213b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
